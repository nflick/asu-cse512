Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/03/18 18:46:37 INFO SecurityManager: Changing view acls to: nathan
15/03/18 18:46:37 INFO SecurityManager: Changing modify acls to: nathan
15/03/18 18:46:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(nathan); users with modify permissions: Set(nathan)
15/03/18 18:46:38 INFO Slf4jLogger: Slf4jLogger started
15/03/18 18:46:38 INFO Remoting: Starting remoting
15/03/18 18:46:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@master:51682]
15/03/18 18:46:38 INFO Utils: Successfully started service 'sparkDriver' on port 51682.
15/03/18 18:46:38 INFO SparkEnv: Registering MapOutputTracker
15/03/18 18:46:38 INFO SparkEnv: Registering BlockManagerMaster
15/03/18 18:46:39 INFO DiskBlockManager: Created local directory at /tmp/spark-local-20150318184639-7432
15/03/18 18:46:39 INFO MemoryStore: MemoryStore started with capacity 267.3 MB
15/03/18 18:46:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/03/18 18:46:39 INFO HttpFileServer: HTTP File server directory is /tmp/spark-064635ff-7620-4648-84a5-745fa6bb7afd
15/03/18 18:46:39 INFO HttpServer: Starting HTTP Server
15/03/18 18:46:39 INFO Utils: Successfully started service 'HTTP file server' on port 47091.
15/03/18 18:46:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/03/18 18:46:40 INFO SparkUI: Started SparkUI at http://master:4040
15/03/18 18:46:42 INFO SparkContext: Added JAR /home/nathan/Shared/project/geospatial/target/geospatial.jar at http://10.0.0.1:47091/jars/geospatial.jar with timestamp 1426729602679
15/03/18 18:46:42 INFO AppClient$ClientActor: Connecting to master spark://10.0.0.1:7077...
15/03/18 18:46:43 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150318184643-0008
15/03/18 18:46:43 INFO AppClient$ClientActor: Executor added: app-20150318184643-0008/0 on worker-20150318135730-worker2-53922 (worker2:53922) with 1 cores
15/03/18 18:46:43 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150318184643-0008/0 on hostPort worker2:53922 with 1 cores, 512.0 MB RAM
15/03/18 18:46:43 INFO AppClient$ClientActor: Executor added: app-20150318184643-0008/1 on worker-20150318135646-worker3-56601 (worker3:56601) with 1 cores
15/03/18 18:46:43 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150318184643-0008/1 on hostPort worker3:56601 with 1 cores, 512.0 MB RAM
15/03/18 18:46:43 INFO AppClient$ClientActor: Executor updated: app-20150318184643-0008/0 is now LOADING
15/03/18 18:46:43 INFO AppClient$ClientActor: Executor updated: app-20150318184643-0008/1 is now LOADING
15/03/18 18:46:43 INFO AppClient$ClientActor: Executor updated: app-20150318184643-0008/0 is now RUNNING
15/03/18 18:46:43 INFO AppClient$ClientActor: Executor updated: app-20150318184643-0008/1 is now RUNNING
15/03/18 18:46:43 INFO NettyBlockTransferService: Server created on 44893
15/03/18 18:46:43 INFO BlockManagerMaster: Trying to register BlockManager
15/03/18 18:46:43 INFO BlockManagerMasterActor: Registering block manager master:44893 with 267.3 MB RAM, BlockManagerId(<driver>, master, 44893)
15/03/18 18:46:43 INFO BlockManagerMaster: Registered BlockManager
15/03/18 18:46:44 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
15/03/18 18:46:44 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes
15/03/18 18:46:44 INFO MemoryStore: ensureFreeSpace(132516) called with curMem=0, maxMem=280248975
15/03/18 18:46:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 129.4 KB, free 267.1 MB)
15/03/18 18:46:45 INFO MemoryStore: ensureFreeSpace(18512) called with curMem=132516, maxMem=280248975
15/03/18 18:46:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 18.1 KB, free 267.1 MB)
15/03/18 18:46:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on master:44893 (size: 18.1 KB, free: 267.2 MB)
15/03/18 18:46:45 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/03/18 18:46:45 INFO SparkContext: Created broadcast 0 from textFile at Common.java:51
15/03/18 18:46:46 INFO FileInputFormat: Total input paths to process : 1
15/03/18 18:46:46 INFO SparkContext: Starting job: reduce at ClosestPoints.java:41
15/03/18 18:46:46 INFO DAGScheduler: Got job 0 (reduce at ClosestPoints.java:41) with 4 output partitions (allowLocal=false)
15/03/18 18:46:46 INFO DAGScheduler: Final stage: Stage 0(reduce at ClosestPoints.java:41)
15/03/18 18:46:46 INFO DAGScheduler: Parents of final stage: List()
15/03/18 18:46:47 INFO DAGScheduler: Missing parents: List()
15/03/18 18:46:47 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@worker3:36936/user/Executor#-1520428392] with ID 1
15/03/18 18:46:47 INFO DAGScheduler: Submitting Stage 0 (FilteredRDD[5] at filter at ClosestPoints.java:40), which has no missing parents
15/03/18 18:46:47 INFO MemoryStore: ensureFreeSpace(4048) called with curMem=151028, maxMem=280248975
15/03/18 18:46:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.0 KB, free 267.1 MB)
15/03/18 18:46:47 INFO MemoryStore: ensureFreeSpace(2786) called with curMem=155076, maxMem=280248975
15/03/18 18:46:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 267.1 MB)
15/03/18 18:46:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master:44893 (size: 2.7 KB, free: 267.2 MB)
15/03/18 18:46:47 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/03/18 18:46:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/03/18 18:46:47 INFO DAGScheduler: Submitting 4 missing tasks from Stage 0 (FilteredRDD[5] at filter at ClosestPoints.java:40)
15/03/18 18:46:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
15/03/18 18:46:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, worker3, NODE_LOCAL, 1559 bytes)
15/03/18 18:46:47 INFO BlockManagerMasterActor: Registering block manager worker3:46290 with 267.3 MB RAM, BlockManagerId(1, worker3, 46290)
15/03/18 18:46:48 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@worker2:55002/user/Executor#1676779999] with ID 0
15/03/18 18:46:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:46:48 INFO BlockManagerMasterActor: Registering block manager worker2:40585 with 267.3 MB RAM, BlockManagerId(0, worker2, 40585)
15/03/18 18:47:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on worker2:40585 (size: 2.7 KB, free: 267.3 MB)
15/03/18 18:47:10 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:47:10 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, worker2): java.lang.ClassCastException: cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2083)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1261)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1996)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

15/03/18 18:47:10 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 3, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:47:10 INFO TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) on executor worker2: java.lang.ClassCastException (cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29) [duplicate 1]
15/03/18 18:47:10 INFO TaskSetManager: Starting task 2.1 in stage 0.0 (TID 4, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:47:10 INFO TaskSetManager: Lost task 1.1 in stage 0.0 (TID 3) on executor worker2: java.lang.ClassCastException (cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29) [duplicate 2]
15/03/18 18:47:10 INFO TaskSetManager: Starting task 1.2 in stage 0.0 (TID 5, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:47:10 INFO TaskSetManager: Lost task 2.1 in stage 0.0 (TID 4) on executor worker2: java.lang.ClassCastException (cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29) [duplicate 3]
15/03/18 18:47:10 INFO TaskSetManager: Starting task 2.2 in stage 0.0 (TID 6, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:47:10 INFO TaskSetManager: Lost task 1.2 in stage 0.0 (TID 5) on executor worker2: java.lang.ClassCastException (cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29) [duplicate 4]
15/03/18 18:47:10 INFO TaskSetManager: Starting task 1.3 in stage 0.0 (TID 7, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:47:10 INFO TaskSetManager: Lost task 2.2 in stage 0.0 (TID 6) on executor worker2: java.lang.ClassCastException (cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29) [duplicate 5]
15/03/18 18:47:10 INFO TaskSetManager: Starting task 2.3 in stage 0.0 (TID 8, worker2, NODE_LOCAL, 1710 bytes)
15/03/18 18:47:10 INFO TaskSetManager: Lost task 1.3 in stage 0.0 (TID 7) on executor worker2: java.lang.ClassCastException (cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29) [duplicate 6]
15/03/18 18:47:10 ERROR TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job
15/03/18 18:47:10 INFO TaskSetManager: Lost task 2.3 in stage 0.0 (TID 8) on executor worker2: java.lang.ClassCastException (cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29) [duplicate 7]
15/03/18 18:47:10 INFO TaskSchedulerImpl: Cancelling stage 0
15/03/18 18:47:11 INFO TaskSchedulerImpl: Stage 0 was cancelled
15/03/18 18:47:11 INFO DAGScheduler: Job 0 failed: reduce at ClosestPoints.java:41, took 24.285111 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, worker2): java.lang.ClassCastException: cannot assign instance of org.apache.spark.rdd.RDD$$anonfun$18 to field org.apache.spark.SparkContext$$anonfun$29.processPartition$1 of type scala.Function1 in instance of org.apache.spark.SparkContext$$anonfun$29
	at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2083)
	at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1261)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1996)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

